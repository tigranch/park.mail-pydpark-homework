{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>420</td><td>application_1544538869062_0689</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://dwhcoursecluster2018-spark-head-hdp-v26-7-0.novalocal:8088/proxy/application_1544538869062_0689/\">Link</a></td><td><a target=\"_blank\" href=\"http://dwhcoursecluster2018-worker-2.novalocal:8042/node/containerlogs/container_e07_1544538869062_0689_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "user_data_path = \"/user/t.chahoyan/data_1/trainDemography\"\n",
    "country_data_path = \"/user/t.chahoyan/data_1/geography/countries.csv\"\n",
    "current_dt = \"2019-05-01\"\n",
    "output_path = \"/user/t.chahoyan/hometask_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType, LongType, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#таблица пользователей\n",
    "\n",
    "schema_user = StructType([\n",
    "    StructField(\"userId\", IntegerType()),\n",
    "    StructField(\"create_date\", LongType()),\n",
    "    StructField(\"birth_date\", IntegerType()),\n",
    "    StructField(\"gender\", IntegerType()),\n",
    "    StructField(\"ID_country\", LongType()),\n",
    "    StructField(\"ID_Location\", IntegerType()),\n",
    "    StructField(\"loginRegion\", IntegerType()),\n",
    "    StructField(\"isInGraph\", IntegerType())\n",
    "])\n",
    "\n",
    "df_user = spark.read.schema(schema_user).csv(user_data_path, sep='\\t')\n",
    "df_user = df_user.select([\"ID_country\", \"userId\", \"birth_date\", \"gender\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# таблица стран\n",
    "\n",
    "schema_countries = StructType([\n",
    "    StructField(\"ID_country\", LongType()),\n",
    "    StructField(\"name_country\", StringType()),\n",
    "])\n",
    "\n",
    "df_countries = spark.read.schema(schema_countries).csv(country_data_path, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, avg, col, year, count, desc\n",
    "from pyspark.sql.functions import round as F_round\n",
    "\n",
    "# из таблицы пользователей вытаскиваем id страны и дату рождения; \n",
    "# берём текущую дату и 1970-01-01, от которой ведётся отсчёт в исходных данных;\n",
    "# с помощью функции date_sub считаем дату рождения пользователя \n",
    "# (так как birth_date - количество дней от 1970-01-01 - так сказано в документации);\n",
    "# с помощью функции datediff считаем разницу между текущей датой и датой рождения - получаем возраст;\n",
    "# далее с помощью функции year получаем год от возраста;\n",
    "# далее группируем по id страны и подсчитываем количество пользователей из данной страны и средний возраст;\n",
    "# результат записываем в first_part\n",
    "\n",
    "\n",
    "first_part = df_user \\\n",
    "    .select(\"ID_country\", \"birth_date\") \\\n",
    "    .withColumn(\"static_date\", lit(\"1970-01-01\").cast(\"date\")) \\\n",
    "    .withColumn(\"current_date\", lit(current_dt).cast(\"date\")) \\\n",
    "    .selectExpr('*', 'date_sub(static_date, -birth_date) as born') \\\n",
    "    .selectExpr('*', 'datediff(current_date, born) as age') \\\n",
    "    .withColumn(\"age\", year(col(\"current_date\")) - year(col(\"born\"))) \\\n",
    "    .groupby(\"ID_country\") \\\n",
    "    .agg(count(\"ID_country\").alias(\"user_cnt\"), avg(\"age\").alias(\"age_avg\")) \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# из таблицы пользователей вытаскиваем id страны и пол, группируем по id страны;\n",
    "# подсчитываем количество мужчин и женщин для каждой страны; результат записываем в second_part\n",
    "\n",
    "second_part = df_user \\\n",
    "          .select([\"ID_country\", \"gender\"]) \\\n",
    "          .groupby(\"ID_country\") \\\n",
    "          .pivot(\"gender\") \\\n",
    "          .count() \\\n",
    "          .select(\"ID_country\", col(\"1\").alias(\"men_cnt\"), col(\"2\").alias(\"women_cnt\"))\n",
    "\n",
    "# объединяем таблицы first_part и second_part по id стран;\n",
    "# находим долю мужчин и долю женщин, округляя до 2 знака после запятой;\n",
    "# округляем средний возраст до 2 знака после запятой;\n",
    "# выводим в нужном порядке\n",
    "\n",
    "result = first_part.join(second_part, \"ID_country\") \\\n",
    ".join(df_countries, \"ID_country\") \\\n",
    ".withColumn(\"men_share\", F_round(col(\"men_cnt\") / col(\"user_cnt\"), 2)) \\\n",
    ".withColumn(\"women_share\", F_round(col(\"women_cnt\") / col(\"user_cnt\"), 2)) \\\n",
    ".withColumn(\"age_avg\", F_round(col(\"age_avg\"), 2)) \\\n",
    ".select([\"name_country\", \"user_cnt\", \"age_avg\", \"men_cnt\", \"women_cnt\", \"men_share\", \"women_share\"]) \\\n",
    ".orderBy(col(\"user_cnt\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# выгружаем результат на hdfs одним .csv файлом\n",
    "\n",
    "result \\\n",
    ".sortWithinPartitions('user_cnt', ascending = False) \\\n",
    ".repartition(1) \\\n",
    ".write.csv(output_path, mode = 'overwrite', sep = '\\t', header = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
